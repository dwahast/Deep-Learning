{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP PyTorch - Template.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwahast/Deep-learning/blob/master/MLP_PyTorch_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T_RsmTkQ98Vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Objetivos deste trabalho\n",
        "- Familiarizar-se com a biblioteca PyTorch\n",
        "- Definir arquiteturas MLP simples em PyTorch\n",
        "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
        "- Comparar os resultados obtidos utilizando apenas Perpceptrons\n",
        "- Link útil (https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c)"
      ]
    },
    {
      "metadata": {
        "id": "qUwhuYBi98V2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn.functional as F # softmax\n",
        "\n",
        "#import pandas as pd # organize files \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LU2fLgnR98WA",
        "colab_type": "code",
        "outputId": "7749d80f-a3ca-4209-e534-2d9c89e77f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Carregar os datasets\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-o4YkPr98WI",
        "colab_type": "code",
        "outputId": "6d2bcde5-1dd5-4057-f0c2-a549024347a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True)\n",
        "test_loader = DataLoader(dataset=dataset_test, shuffle=False)\n",
        "print(len(test_loader))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L36yfjGr98WO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura MLP\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.activation_function = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = self.activation_function(self.fc1(x))\n",
        "        x = self.activation_function(self.fc2(x))\n",
        "        return x\n",
        "      \n",
        "class MLP_relu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_relu, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.log_softmax(self.fc2(x),dim =1)\n",
        "        return x\n",
        "      \n",
        "      \n",
        "class MLP_Srelu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_Srelu, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 32)\n",
        "        self.fc4 = nn.Linear(32, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.log_softmax(self.fc4(x),dim =1)\n",
        "        return x   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DSF3ZnQ98WT",
        "colab_type": "code",
        "outputId": "106dca41-5907-4ad2-848b-e664c4828447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "model = MLP_Srelu()\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_Srelu(\n",
            "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rh1fBZcW98WZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir otimizador e loss\n",
        "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
        "\n",
        "def opt_lss_lr_set(optimizer_method, loss_function, learning_rate):\n",
        "   \n",
        "  if loss_function == \"mse\":\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "  elif loss_function == \"nllos\":\n",
        "     loss_fn = torch.nn.NLLoss()\n",
        "  elif loss_function == \"cross\":\n",
        "     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  if optimizer_method == \"sgd\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.5)\n",
        "  elif optimizer_method == \"adadelta\":\n",
        "    optimizer = torch.optim.Adadelta(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_method == \"adagrad\":\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_method == \"adamax\":\n",
        "    optimizer = torch.optim.Adamax(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_method == \"rms\":\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "  \n",
        "  print(\"Opt Method:\", optimizer_method.upper(), \"| Loss Function:\", loss_function.upper(), \"| Learning Rate:\", learning_rate)\n",
        "  \n",
        "  return optimizer, loss_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZG-R1kCQEweC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(label, output_size):\n",
        "    \n",
        "    label_select = np.zeros(output_size)\n",
        "    label_select[label]=1\n",
        "    \n",
        "    return torch.Tensor(label_select)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p87aRLgR2pPo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_fn, loss_function):\n",
        "  model.eval() # set model to Evaluate \"mode\"\n",
        "  losses = []\n",
        "  corrects = 0\n",
        "  current_total = 0\n",
        "  accuracies = []\n",
        "  \n",
        "  for image, label in test_loader:\n",
        "    weights = model(image)\n",
        "   \n",
        "    if(loss_function == 'mse'):\n",
        "      label_target = one_hot(label,10) # one_hot to set the training class in the escalar\n",
        "      loss = loss_fn(weights,label_target)\n",
        "    else:\n",
        "      loss = loss_fn(weights,label)\n",
        "      \n",
        "    losses.append(loss.item())\n",
        "    _, predicted = torch.max(weights.data, 1)\n",
        "    if(predicted==label):\n",
        "      corrects += 1\n",
        "    \n",
        "    current_total += 1\n",
        "    accuracies.append(corrects/current_total)\n",
        " \n",
        " # print(\"Mean Accuracy: \", np.mean(accuracies))\n",
        "  return np.mean(accuracies), np.mean(losses)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MELtsUbB98Wf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Realizar o treinamento aqui\n",
        "def fit(epochs, optimizer_method, loss_function, learning_rate):\n",
        " \n",
        "  optimizer, loss_fn = opt_lss_lr_set(optimizer_method, loss_function, learning_rate) #optimizer loss and Learning rate setter\n",
        "  Accuracies = []\n",
        "  Test_losses = []\n",
        "  losses = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train() # Set model to TRAIN \"mode\" (can be set to False for Test)\n",
        "    epoch_losses = []\n",
        "    for image,label in train_loader:\n",
        "      optimizer.zero_grad()  # cleaning gradients between mini batches\n",
        "      weights = model(image) \n",
        "     \n",
        "      if(loss_function == 'mse'):\n",
        "        label_target = one_hot(label,10) # one_hot to set the training class in the escalar\n",
        "        loss = loss_fn(weights,label_target)\n",
        "      else: \n",
        "        loss = loss_fn(weights, label)\n",
        "        \n",
        "      loss.backward() # Backpropagation \n",
        "      optimizer.step() # Optimization Method\n",
        "      \n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    losses.append(np.mean(epoch_losses)) # Append mean losses for each epoch\n",
        "   \n",
        "    acc, test_loss = evaluate(model, loss_fn, loss_function)\n",
        "    Accuracies.append(acc)\n",
        "    Test_losses.append(test_loss)\n",
        "    if(epoch%10==0):\n",
        "      print(\"Epoch:\",epoch, \"- Average loss:\", np.mean(epoch_losses),\"- Accuracy:\", acc) # Print mean Loss for each epoch\n",
        "    \n",
        "  \n",
        "  print(\"Mean Accuracies:\",np.mean(Accuracies))\n",
        "  plt.title(\"Accuracies\")\n",
        "  plt.plot(Accuracies)\n",
        "  plt.show()\n",
        "  plt.title(\"Test Loss\")\n",
        "  plt.plot(Test_losses)\n",
        "  plt.show()\n",
        " \n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kAvty-S98Wk",
        "colab_type": "code",
        "outputId": "2bc97049-85c4-497c-9a20-ec11ab59a5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo aqui (no conjunto de teste)\n",
        "fit(15,\"sgd\",    \"mse\", 0.01)\n",
        "fit(15,\"adadelta\",   \"mse\", 0.01)\n",
        "fit(15,\"rms\",    \"mse\", 0.01)\n",
        "fir(15,\"adagrad\", \"mse\", 0.01)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7ea9dfe301b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"adadelta\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rms\"\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"adagrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2b81afd7eb50>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, optimizer_method, loss_function, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Optimization Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
